{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efde70e-6d5e-4f8c-9c09-2e944f1e38dd",
   "metadata": {},
   "source": [
    "            ENCODER:\n",
    "\n",
    "\n",
    "\n",
    "                      Input Embeddings\n",
    "                             |\n",
    "                             V\n",
    "                    +--------------------+\n",
    "                    | Positional Encoding|\n",
    "                    +--------------------+\n",
    "                             |\n",
    "                             V\n",
    "             +----------------------------------+\n",
    "             |                                  |\n",
    "             |  Multi-Head Self-Attention       |\n",
    "             |    +------------------------+    |\n",
    "             |    | Query (Q)              |    |\n",
    "             |    | Key (K)                |    |\n",
    "             |    | Value (V)              |    |\n",
    "             |    | (from Input + Positional) | |\n",
    "             |    +------------------------+    |\n",
    "             |            |                     |\n",
    "             |            V                     |\n",
    "             |   Scaled Dot-Product Attention   |\n",
    "             |   (Q@K.T / sqrt(d_k))            |\n",
    "             |            |                     |\n",
    "             |            V                     |\n",
    "             |         Softmax                  |\n",
    "             |            |                     |\n",
    "             |            V                     |\n",
    "             |   (Attention Weights @ V)        |\n",
    "             |            |                     |\n",
    "             |            V                     |\n",
    "             |    +------------------------+    |\n",
    "             |    | Concat Heads & Linear  |    |\n",
    "             |    +------------------------+    |\n",
    "             +----------------------------------+\n",
    "                             |\n",
    "                             V\n",
    "                    +--------------------+\n",
    "                    |  Add & Normalize   |\n",
    "                    | (Residual Conn. &  |\n",
    "                    |   Layer Norm)      |\n",
    "                    +--------------------+\n",
    "                             |\n",
    "                             V\n",
    "                    +--------------------+\n",
    "                    |  Feed-Forward Net  |\n",
    "                    | (Position-wise FFN)|\n",
    "                    +--------------------+\n",
    "                             |\n",
    "                             V\n",
    "                    +--------------------+\n",
    "                    |  Add & Normalize   |\n",
    "                    | (Residual Conn. &  |\n",
    "                    |   Layer Norm)      |\n",
    "                    +--------------------+\n",
    "                             |\n",
    "                             V\n",
    "                         Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371757a-ddc5-47a1-9d27-48264d91b555",
   "metadata": {},
   "source": [
    "                    Decoder Input Embeddings\n",
    "                               |\n",
    "                               V\n",
    "                      +--------------------+\n",
    "                      | Positional Encoding|\n",
    "                      +--------------------+\n",
    "                               |\n",
    "                               V\n",
    "               +----------------------------------+\n",
    "               |                                  |\n",
    "               |  Masked Multi-Head Self-Attention|\n",
    "               |    +------------------------+    |\n",
    "               |    | Query (Q)              |    |\n",
    "               |    | Key (K)                |    |\n",
    "               |    | Value (V)              |    |\n",
    "               |    | (from Decoder Input + Positional) |\n",
    "               |    +------------------------+    |\n",
    "               |            |                     |\n",
    "               |            V                     |\n",
    "               |   Scaled Dot-Product Attention   |\n",
    "               |   (Q@K.T / sqrt(d_k))            |\n",
    "               |            |                     |\n",
    "               |            V                     |\n",
    "               |         Masking                  |\n",
    "               |            |                     |\n",
    "               |            V                     |\n",
    "               |         Softmax                  |\n",
    "               |            |                     |\n",
    "               |            V                     |\n",
    "               |   (Attention Weights @ V)        |\n",
    "               |            |                     |\n",
    "               |            V                     |\n",
    "               |    +------------------------+    |\n",
    "               |    | Concat Heads & Linear  |    |\n",
    "               |    +------------------------+    |\n",
    "               +----------------------------------+\n",
    "                               |\n",
    "                               V\n",
    "                      +--------------------+\n",
    "                      |  Add & Normalize   |\n",
    "                      | (Residual Conn. &  |\n",
    "                      |   Layer Norm)      |\n",
    "                      +--------------------+\n",
    "                               |\n",
    "                               V\n",
    "             Encoder Output -----------------> +----------------------------------+\n",
    "             (K, V)                              |                                  |\n",
    "                                                 |  Multi-Head Encoder-Decoder      |\n",
    "                                                 |        Attention                 |\n",
    "                                                 |    +------------------------+    |\n",
    "                                                 |    | Query (Q) (from prev layer)|\n",
    "                                                 |    | Key (K) (from Encoder) |    |\n",
    "                                                 |    | Value (V) (from Encoder)|   |\n",
    "                                                 |    +------------------------+    |\n",
    "                                                 |            |                     |\n",
    "                                                 |            V                     |\n",
    "                                                 |   Scaled Dot-Product Attention   |\n",
    "                                                 |   (Q@K.T / sqrt(d_k))            |\n",
    "                                                 |            |                     |\n",
    "                                                 |            V                     |\n",
    "                                                 |         Softmax                  |\n",
    "                                                 |            |                     |\n",
    "                                                 |            V                     |\n",
    "                                                 |   (Attention Weights @ V)        |\n",
    "                                                 |            |                     |\n",
    "                                                 |            V                     |\n",
    "                                                 |    +------------------------+    |\n",
    "                                                 |    | Concat Heads & Linear  |    |\n",
    "                                                 |    +------------------------+    |\n",
    "                                                 +----------------------------------+\n",
    "                                                               |\n",
    "                                                               V\n",
    "                                                      +--------------------+\n",
    "                                                      |  Add & Normalize   |\n",
    "                                                      | (Residual Conn. &  |\n",
    "                                                      |   Layer Norm)      |\n",
    "                                                      +--------------------+\n",
    "                                                               |\n",
    "                                                               V\n",
    "                                                      +--------------------+\n",
    "                                                      |  Feed-Forward Net  |\n",
    "                                                      | (Position-wise FFN)|\n",
    "                                                      +--------------------+\n",
    "                                                               |\n",
    "                                                               V\n",
    "                                                      +--------------------+\n",
    "                                                      |  Add & Normalize   |\n",
    "                                                      | (Residual Conn. &  |\n",
    "                                                      |   Layer Norm)      |\n",
    "                                                      +--------------------+\n",
    "                                                               |\n",
    "                                                               V\n",
    "                                                            Output\n",
    "                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d95f7-16b8-48ec-ba11-bac9cae5b060",
   "metadata": {},
   "source": [
    "Basically:\n",
    "First, the input embeddings are passed through the encoder to create context\n",
    "At the same time, output embeddings are given to the decoder. The decoder uses a masked attention layer to avoid contextualizing subsequent words. \n",
    "That ouput then uses the context that encoder generated(query vectors) to finally generate output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93308669-7be4-4e7f-b34f-15f8f3c269bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
